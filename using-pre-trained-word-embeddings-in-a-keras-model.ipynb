{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach\n",
    "Here's how we will solve the classification problem:\n",
    "\n",
    "- convert all text samples in the dataset into sequences of word indices. A \"word index\" would simply be an integer ID for the word. We will only consider the top 20,000 most commonly occuring words in the dataset, and we will truncate the sequences to a maximum length of 1000 words.\n",
    "- prepare an \"embedding matrix\" which will contain at index i the embedding vector for the word of index i in our word index.\n",
    "- load this embedding matrix into a Keras Embedding layer, set to be frozen (its weights, the embedding vectors, will not be updated during training).\n",
    "- build on top of it a 1D convolutional neural network, ending in a softmax output over our 20 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sun\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '../pre_trained word embeddings'\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\n",
    "TEXT_DATA_DIR = os.path.join('./data')\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the text data\n",
    "First, we will simply iterate over the folders in which our text samples are stored, and format them into a list of samples. We will also prepare at the same time a list of class indices matching the samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sys.version_info(major=3, minor=6, micro=4, releaselevel='final', serial=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.version_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19997 texts.\n"
     ]
    }
   ],
   "source": [
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    if os.path.isdir(path):\n",
    "        label_id = len(labels_index)\n",
    "        labels_index[name] = label_id\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            if fname.isdigit():\n",
    "                fpath = os.path.join(path, fname)\n",
    "                args = {} if sys.version_info < (3,) else {'encoding': 'latin-1'}\n",
    "                with open(fpath, **args) as f:\n",
    "                    t = f.read()\n",
    "                    i = t.find('\\n\\n')  # skip header\n",
    "                    if 0 < i:\n",
    "                        t = t[i:]\n",
    "                    texts.append(t)\n",
    "                labels.append(label_id)\n",
    "\n",
    "print('Found %s texts.' % len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can format our text samples and labels into tensors that can be fed into a neural network. To do this, we will rely on Keras utilities keras.preprocessing.text.Tokenizer and keras.preprocessing.sequence.pad_sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 174074 unique tokens.\n",
      "Shape of data tensor: (19997, 1000)\n",
      "Shape of label tensor: (19997, 20)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Embedding layer\n",
    "Next, we compute an index mapping words to known embeddings, by parsing the data dump of pre-trained embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we can leverage our embedding_index dictionary and our word_index to compute our embedding matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load this embedding matrix into an Embedding layer. Note that we set trainable=False to prevent the weights from being updated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not using pre-trained word embeddings\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pre-trained word embeddings\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tuning the Embedding layer\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Embedding layer should be fed sequences of integers, i.e. a 2D input of shape (samples, indices). These input sequences should be padded so that they all have the same length in a batch of input data (although an Embedding layer is capable of processing sequence of heterogenous length, if you don't pass an explicit input_length argument to the layer).\n",
    "\n",
    "All that the Embedding layer does is to map the integer inputs to the vectors found at the corresponding index in the embedding matrix, i.e. the sequence [1, 2] would be converted to [embeddings[1], embeddings[2]]. This means that the output of the Embedding layer will be a 3D tensor of shape (samples, sequence_length, embedding_dim)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a 1D convnet\n",
    "Finally we can then build a small 1D convnet to solve our classification problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 1000, 100)         17407500  \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 996, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 199, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 195, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 39, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 35, 128)           82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 20)                2580      \n",
      "=================================================================\n",
      "Total params: 17,654,816\n",
      "Trainable params: 17,654,816\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/10\n",
      "15998/15998 [==============================] - 9s 557us/step - loss: 2.7856 - acc: 0.0885 - val_loss: 2.3112 - val_acc: 0.1653\n",
      "Epoch 2/10\n",
      "15998/15998 [==============================] - 8s 506us/step - loss: 2.1706 - acc: 0.2055 - val_loss: 2.0157 - val_acc: 0.2843\n",
      "Epoch 3/10\n",
      "15998/15998 [==============================] - 8s 508us/step - loss: 1.8088 - acc: 0.3484 - val_loss: 1.8570 - val_acc: 0.3733\n",
      "Epoch 4/10\n",
      "15998/15998 [==============================] - 8s 506us/step - loss: 1.3140 - acc: 0.5388 - val_loss: 1.3600 - val_acc: 0.5431\n",
      "Epoch 5/10\n",
      "15998/15998 [==============================] - 8s 508us/step - loss: 1.0204 - acc: 0.6410 - val_loss: 1.2136 - val_acc: 0.5954\n",
      "Epoch 6/10\n",
      "15998/15998 [==============================] - 8s 509us/step - loss: 0.8006 - acc: 0.7304 - val_loss: 1.5081 - val_acc: 0.5416\n",
      "Epoch 7/10\n",
      "15998/15998 [==============================] - 8s 509us/step - loss: 0.6209 - acc: 0.7945 - val_loss: 1.1109 - val_acc: 0.6792\n",
      "Epoch 8/10\n",
      "15998/15998 [==============================] - 8s 508us/step - loss: 0.4813 - acc: 0.8453 - val_loss: 1.1604 - val_acc: 0.6799\n",
      "Epoch 9/10\n",
      "15998/15998 [==============================] - 8s 509us/step - loss: 0.3753 - acc: 0.8860 - val_loss: 1.1309 - val_acc: 0.6982\n",
      "Epoch 10/10\n",
      "15998/15998 [==============================] - 8s 509us/step - loss: 0.3199 - acc: 0.9014 - val_loss: 1.1591 - val_acc: 0.7147\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x178b1a72048>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# not using pre-trained word embeddings\n",
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1000, 100)         2000000   \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 996, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 199, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 195, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 39, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 35, 128)           82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 20)                2580      \n",
      "=================================================================\n",
      "Total params: 2,247,316\n",
      "Trainable params: 247,316\n",
      "Non-trainable params: 2,000,000\n",
      "_________________________________________________________________\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/10\n",
      "15998/15998 [==============================] - 10s 621us/step - loss: 2.4348 - acc: 0.2007 - val_loss: 2.0071 - val_acc: 0.2878 2.4391 - acc: 0.199\n",
      "Epoch 2/10\n",
      "15998/15998 [==============================] - 5s 310us/step - loss: 1.6035 - acc: 0.4368 - val_loss: 1.3785 - val_acc: 0.5089\n",
      "Epoch 3/10\n",
      "15998/15998 [==============================] - 5s 311us/step - loss: 1.2112 - acc: 0.5804 - val_loss: 1.1232 - val_acc: 0.6292\n",
      "Epoch 4/10\n",
      "15998/15998 [==============================] - 5s 311us/step - loss: 0.9717 - acc: 0.6702 - val_loss: 1.0255 - val_acc: 0.6469\n",
      "Epoch 5/10\n",
      "15998/15998 [==============================] - 5s 312us/step - loss: 0.8225 - acc: 0.7253 - val_loss: 1.0068 - val_acc: 0.6734\n",
      "Epoch 6/10\n",
      "15998/15998 [==============================] - 5s 312us/step - loss: 0.6964 - acc: 0.7659 - val_loss: 1.0052 - val_acc: 0.6802\n",
      "Epoch 7/10\n",
      "15998/15998 [==============================] - 5s 313us/step - loss: 0.5896 - acc: 0.8037 - val_loss: 0.8843 - val_acc: 0.713238 -  - ETA: 1s -\n",
      "Epoch 8/10\n",
      "15998/15998 [==============================] - 5s 316us/step - loss: 0.5006 - acc: 0.8309 - val_loss: 1.0707 - val_acc: 0.6724\n",
      "Epoch 9/10\n",
      "15998/15998 [==============================] - 5s 315us/step - loss: 0.4186 - acc: 0.8582 - val_loss: 0.9722 - val_acc: 0.7187\n",
      "Epoch 10/10\n",
      "15998/15998 [==============================] - 5s 313us/step - loss: 0.3617 - acc: 0.8805 - val_loss: 1.0231 - val_acc: 0.7122\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17896c3ca58>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using pre-trained word embeddings\n",
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 1000, 100)         2000000   \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 996, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 199, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 195, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 39, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 35, 128)           82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_4 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 20)                2580      \n",
      "=================================================================\n",
      "Total params: 2,247,316\n",
      "Trainable params: 2,247,316\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/20\n",
      "15998/15998 [==============================] - 7s 458us/step - loss: 2.3917 - acc: 0.2117 - val_loss: 1.8665 - val_acc: 0.3506\n",
      "Epoch 2/20\n",
      "15998/15998 [==============================] - 7s 412us/step - loss: 1.4384 - acc: 0.4999 - val_loss: 1.2473 - val_acc: 0.5749\n",
      "Epoch 3/20\n",
      "15998/15998 [==============================] - 7s 409us/step - loss: 1.0036 - acc: 0.6603 - val_loss: 1.0161 - val_acc: 0.6552\n",
      "Epoch 4/20\n",
      "15998/15998 [==============================] - 7s 411us/step - loss: 0.7616 - acc: 0.7457 - val_loss: 0.9159 - val_acc: 0.6947\n",
      "Epoch 5/20\n",
      "15998/15998 [==============================] - 7s 411us/step - loss: 0.6073 - acc: 0.7973 - val_loss: 0.8098 - val_acc: 0.7414\n",
      "Epoch 6/20\n",
      "15998/15998 [==============================] - 7s 409us/step - loss: 0.4849 - acc: 0.8382 - val_loss: 0.7308 - val_acc: 0.7654\n",
      "Epoch 7/20\n",
      "15998/15998 [==============================] - 7s 409us/step - loss: 0.3845 - acc: 0.8718 - val_loss: 0.8189 - val_acc: 0.7654\n",
      "Epoch 8/20\n",
      "15998/15998 [==============================] - 7s 411us/step - loss: 0.3050 - acc: 0.8964 - val_loss: 0.8171 - val_acc: 0.7599\n",
      "Epoch 9/20\n",
      "15998/15998 [==============================] - 7s 411us/step - loss: 0.2476 - acc: 0.9191 - val_loss: 0.7785 - val_acc: 0.7867\n",
      "Epoch 10/20\n",
      "15998/15998 [==============================] - 7s 411us/step - loss: 0.2066 - acc: 0.9296 - val_loss: 0.9909 - val_acc: 0.7464\n",
      "Epoch 11/20\n",
      "15998/15998 [==============================] - 7s 412us/step - loss: 0.1740 - acc: 0.9414 - val_loss: 0.9738 - val_acc: 0.7747\n",
      "Epoch 12/20\n",
      "15998/15998 [==============================] - 7s 415us/step - loss: 0.1537 - acc: 0.9457 - val_loss: 0.8723 - val_acc: 0.7844\n",
      "Epoch 13/20\n",
      "15998/15998 [==============================] - 7s 413us/step - loss: 0.1384 - acc: 0.9533 - val_loss: 0.9191 - val_acc: 0.7859\n",
      "Epoch 14/20\n",
      "15998/15998 [==============================] - 7s 413us/step - loss: 0.1297 - acc: 0.9542 - val_loss: 0.9437 - val_acc: 0.7954\n",
      "Epoch 15/20\n",
      "15998/15998 [==============================] - 7s 411us/step - loss: 0.1214 - acc: 0.9595 - val_loss: 0.8957 - val_acc: 0.7874\n",
      "Epoch 16/20\n",
      "15998/15998 [==============================] - 7s 412us/step - loss: 0.1144 - acc: 0.9596 - val_loss: 0.9695 - val_acc: 0.7942\n",
      "Epoch 17/20\n",
      "15998/15998 [==============================] - 7s 412us/step - loss: 0.1084 - acc: 0.9599 - val_loss: 1.0637 - val_acc: 0.7839\n",
      "Epoch 18/20\n",
      "15998/15998 [==============================] - 7s 411us/step - loss: 0.0994 - acc: 0.9637 - val_loss: 1.0675 - val_acc: 0.7857\n",
      "Epoch 19/20\n",
      "15998/15998 [==============================] - 7s 412us/step - loss: 0.0970 - acc: 0.9629 - val_loss: 1.0311 - val_acc: 0.7932\n",
      "Epoch 20/20\n",
      "15998/15998 [==============================] - 7s 416us/step - loss: 0.0892 - acc: 0.9633 - val_loss: 1.2783 - val_acc: 0.7429\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1789fbfab70>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fine-tuning the Embedding layer\n",
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=20,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
